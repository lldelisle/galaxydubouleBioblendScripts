# galaxydubouleBioblendScripts

## Python scripts using bioblend to interact with galaxy more automatically

These python scripts are using [BioBlend](https://bioblend.readthedocs.io/) to interact with galaxy.

## Usage

As it uses BioBlend, you need to install it. The easiest way is to use conda:

```bash
$ conda create -n bioblend -c bioconda bioblend "python>=3.7"
```

Then activate your environment:

```bash
$ conda activate bioblend
```

Then you need to download the scripts, you can click on Code, Download ZIP on the upper part of the page and then unzip it. You will set the path of galaxydubouleBioblendScripts in a bash variable:
```bash
# You need to adapt to where you downloaded:
gitHubDirectory=/home/ldelisle/Documents/mygit/galaxydubouleBioblendScripts/
```

In order to use these script you need to get your api key from your galaxy instance.
Log in your galaxy, go to User, Settings, Manage API key. If there is no, you need to generate one. Then copy it and put it in a bash variable:

```bash
# You need to adapt to your API:
myAPI=blablabla
```

To be able to download a list of dataset you need to get their API ID. If you don't have a lot of them, you can get them clicking on the (i) icon on the galaxy webpage, checking the line "History Content API ID".


If you have more, you may be interested to use the first script:

### `get_datasets_id.py`

This script can be used to get all datasets ids from some histories or all histories.

If you want for all histories (not deleted), just do:

```bash
$ python ${gitHubDirectory}/scripts/get_datasets_id.py --api $myAPI > all_my_datasets.txt
```

If you have a lot of them, this can be long...

If you already know which history/ies you want, you need to go to the webpage, click on the (i) icon of one of your dataset of the history and copy the History API ID in a file. It should be one id per line.

Then run:

```bash
$ python ${gitHubDirectory}/scripts/get_datasets_id.py --api $myAPI --historiesTable my_histories.txt > my_datasets_in_my_histories.txt
```

The output file is a tabular delimited file whose first column is the dataset id, the second is the dataset name, the third is the history id and the fourth is the history_name.

You may want to modify this file to remove datasets that you don't want to download.


### `download_datasets.py`

This script can be used to download all datasets from a list of dataset ids.
The ids should be one per line. They can be obtained from the webpage by clicking on the (i) icon and using the History Content API ID or can be obtained with `get_datasets_id.py`.

It will download the datasets in one subdirectory per history and the name will be the one you would have if you would have downloaded it through the webpage.

An example usage is:

```bash
$ python ${gitHubDirectory}/scripts/download_datasets.py --api $myAPI --datasetTable my_histories.txt --outputFolder my_downloads/
```

Depending on the number of datasets, it can be quite long...

## Documentation

Here is the full help:

### `get_datasets_id.py`

```text
usage: get_datasets_id.py [-h] [--url URL] --api API [--historiesTable HISTORIESTABLE] [--deleted] [--output OUTPUT]

Get all items from history ids.

optional arguments:
  -h, --help            show this help message and exit
  --url URL             A FQDN or IP for a given instance of Galaxy.
  --api API             Your API key. Can be obtained from the webpage. User - Settings - Manage API key
  --historiesTable HISTORIESTABLE
                        [Optional] Input table with one line per history. It should correspond to the History API ID.
  --deleted             Get the deleted histories only.
  --output OUTPUT       Output table.
```

### `download_datasets.py`

``` text
usage: download_datasets.py [-h] [--url URL] --api API --datasetTable DATASETTABLE --outputFolder OUTPUTFOLDER

Download datasets from dataset ids.

optional arguments:
  -h, --help            show this help message and exit
  --url URL             A FQDN or IP for a given instance of Galaxy.
  --api API             Your API key. Can be obtained from the webpage. User - Settings - Manage API key
  --datasetTable DATASETTABLE
                        Input table with one line per dataset. It should correspond to the History Content API ID. It can also be
                        generated by get_datasets.py
  --outputFolder OUTPUTFOLDER
                        Folder where files will be downloaded.
```

## Note

By default the `--url` is set to galaxyduboule.epfl.ch but if you are not part of Duboule lab, you can still use the scripts and change it...
